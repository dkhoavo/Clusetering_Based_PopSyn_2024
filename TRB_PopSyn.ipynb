{"cells":[{"cell_type":"markdown","metadata":{"id":"fFqIbvhbJZx9"},"source":["# Memory reduction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jz5nX11lpg8q"},"outputs":[],"source":["# Use to reduce the size of dataframes when needed\n","\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def reduce_mem_usage(props):\n","    start_mem_usg = props.memory_usage().sum() / 1024**2\n","    NAlist = [] # Keeps track of columns that have missing values filled in.\n","    for col in props.columns:\n","        if (props[col].dtype != object) & (props[col].dtype != 'category'):  # Exclude strings\n","\n","            # make variables for Int, max and min\n","            IsInt = False\n","            mx = props[col].max()\n","            mn = props[col].min()\n","\n","            # Integer does not support NA, therefore, NA needs to be filled\n","            if not np.isfinite(props[col]).all():\n","                NAlist.append(col)\n","                props[col].fillna(mn-1,inplace=True)\n","\n","            # test if column can be converted to an integer\n","            asint = props[col].fillna(0).astype(np.int64)\n","            result = (props[col] - asint)\n","            result = result.sum()\n","            if result > -0.01 and result < 0.01:\n","                IsInt = True\n","\n","            if IsInt:\n","                if mn >= 0:\n","                    if mx < 255:\n","                        props[col] = props[col].astype(np.uint8)\n","                    elif mx < 65535:\n","                        props[col] = props[col].astype(np.uint16)\n","                    elif mx < 4294967295:\n","                        props[col] = props[col].astype(np.uint32)\n","                    else:\n","                        props[col] = props[col].astype(np.uint64)\n","                else:\n","                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n","                        props[col] = props[col].astype(np.int8)\n","                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n","                        props[col] = props[col].astype(np.int16)\n","                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n","                        props[col] = props[col].astype(np.int32)\n","                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n","                        props[col] = props[col].astype(np.int64)\n","            else:\n","                props[col] = props[col].astype(np.float32)\n","\n","    mem_usg = props.memory_usage().sum() / 1024**2\n","    return props\n"]},{"cell_type":"markdown","metadata":{"id":"XLKdzn5BrBTl"},"source":["# Datasets"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Yr-g_PNJJspE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732677825030,"user_tz":-480,"elapsed":272,"user":{"displayName":"Khoa Vo","userId":"00040010443663681368"}},"outputId":"87317a76-4d18-4c1f-92bc-d7aaec2611f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 2000 entries, 0 to 1999\n","Data columns (total 12 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Gender      2000 non-null   int64  \n"," 1   Age         2000 non-null   int64  \n"," 2   Homeincome  2000 non-null   int64  \n"," 3   NumHH       2000 non-null   int64  \n"," 4   Distance    2000 non-null   int64  \n"," 5   Home_X      2000 non-null   float64\n"," 6   Home_Y      2000 non-null   float64\n"," 7   Work_X      2000 non-null   float64\n"," 8   Work_Y      2000 non-null   float64\n"," 9   Home_Hcode  2000 non-null   int64  \n"," 10  Work_Hcode  2000 non-null   int64  \n"," 11  Prob        2000 non-null   float64\n","dtypes: float64(5), int64(7)\n","memory usage: 187.6 KB\n","None\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5895 entries, 0 to 5894\n","Data columns (total 8 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Distance    5895 non-null   int64  \n"," 1   Home_X      5895 non-null   float64\n"," 2   Home_Y      5895 non-null   float64\n"," 3   Work_X      5895 non-null   float64\n"," 4   Work_Y      5895 non-null   float64\n"," 5   Home_Hcode  5895 non-null   int64  \n"," 6   Work_Hcode  5895 non-null   int64  \n"," 7   Prob        5895 non-null   float64\n","dtypes: float64(5), int64(3)\n","memory usage: 368.6 KB\n","None\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20000 entries, 0 to 19999\n","Data columns (total 12 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Gender      20000 non-null  int64  \n"," 1   Age         20000 non-null  int64  \n"," 2   Homeincome  20000 non-null  int64  \n"," 3   NumHH       20000 non-null  int64  \n"," 4   Distance    20000 non-null  int64  \n"," 5   Home_X      20000 non-null  float64\n"," 6   Home_Y      20000 non-null  float64\n"," 7   Work_X      20000 non-null  float64\n"," 8   Work_Y      20000 non-null  float64\n"," 9   Cluster     20000 non-null  int64  \n"," 10  Home_Hcode  20000 non-null  int64  \n"," 11  Work_Hcode  20000 non-null  int64  \n","dtypes: float64(4), int64(8)\n","memory usage: 1.8 MB\n","None\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Change the path if needed\n","project_path = ''\n","\n","df_cel = pd.read_csv(project_path + 'data/data_cel_sim.csv')\n","df_hts = pd.read_csv( project_path + 'data/data_hts_sim.csv')\n","df_true = pd.read_csv(project_path + 'data/data_true_sim.csv')\n","\n","print(df_hts.info())\n","print(df_cel.info())\n","print(df_true.info())\n","\n","del df_hts, df_cel, df_true"]},{"cell_type":"markdown","metadata":{"id":"ZFhOuGMzPjFi"},"source":["# Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWeLx0PzazDc"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","from matplotlib import pyplot as plt\n","from sklearn import preprocessing\n","from sklearn.cluster import Birch\n","from sklearn.cluster import AgglomerativeClustering\n","from scipy.special import rel_entr, kl_div\n","from scipy.stats import entropy\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","df_htsz = pd.read_csv(project_path + 'data/data_hts_sim.csv')\n","df_celz = pd.read_csv(project_path + 'data/data_cel_sim.csv')\n","\n","# Attributes\n","att_X = ['Homeincome', 'NumHH', 'Gender', 'Age']\n","att_Z = ['Home_X', 'Home_Y' , 'Work_X', 'Work_Y']\n","att_Y = ['Distance']\n","att_C = ['Cluster']\n","\n","# JS divergence between two distributions p and q\n","def js_div(p, q):\n","    h = 0.5*np.add(p,q)\n","    return 0.5*rel_entr(p, h) + 0.5*rel_entr(q, h)\n","\n","# BRICH clustering, any clustering methods can be adopted\n","def clustering(df, birch_threshold, bf, K, w):\n","    w_ = [w, w, 1-w, 1-w]\n","    X = df * w_\n","    data = preprocessing.MinMaxScaler().fit_transform(X)\n","    brc = Birch(threshold=birch_threshold, branching_factor=bf, n_clusters=K)\n","    brc.fit(data)\n","    labels = brc.labels_\n","    pred = brc.predict(data)\n","    df['Cluster'] = pred\n","    clusters = np.unique(pred)\n","    return df.drop_duplicates()\n","\n","# Auxilary attributes (i.e., distance) based on home and work locations\n","def add_distance(df, N):\n","    lon1 = np.radians(df['Home_X'])\n","    lon2 = np.radians(df['Work_X'])\n","    lat1 = np.radians(df['Home_Y'])\n","    lat2 = np.radians(df['Work_Y'])\n","    dlon = lon2 - lon1\n","    dlat = lat2 - lat1\n","    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n","    df['Distance'] = np.ceil(6371*2 * np.arcsin(np.sqrt(a)))\n","    if (N > 1):\n","        bin_dis = [i*int(12/N)  for i in range(N+1)] + [25,99999]\n","        label_dis = [i for i in range(1,len(bin_dis)-1)] + [99]\n","    else:\n","        bin_dis = [0,99999]\n","        label_dis = [1]\n","    df['Distance'] = pd.cut(df['Distance'], bins=bin_dis, labels=label_dis, include_lowest=True)\n","\n","# Calculate entropy of a distribution\n","def calculate_entropy(df_celzz):\n","    df_celzzk_ = df_celzz[att_Y + att_C + att_Z + ['Prob_cel']]\n","    df_celzzk_['Prob_cel_tt'] = df_celzzk_.groupby(att_Y + att_C)['Prob_cel'].transform('sum')\n","    df_celzzk_['Count'] = 2.0\n","    df_celzzk_['Prob_cel'] = df_celzzk_['Prob_cel'] / df_celzzk_['Prob_cel_tt']\n","    df_celzzk_['entropy'] = df_celzzk_['Prob_cel_tt'] * (-df_celzzk_['Prob_cel'] * np.emath.logn(df_celzzk_['Count'], df_celzzk_['Prob_cel']))\n","    return df_celzzk_[['entropy']].sum()[0]\n","\n","\n","# parato check\n","def find_prareto(sols, sol):\n","    offset = 4\n","    sols_ = []\n","    if len(sols) == 0:\n","        sols_ = [sol]\n","    for en in sols:\n","        if (sol[offset] < en[offset]) & (sol[offset+1] < en[offset+1]):\n","            if sol not in sols_:\n","                sols_.append(sol)\n","        elif (sol[offset] <= en[offset]) & (sol[offset+1] < en[offset+1]):\n","            if sol not in sols_:\n","                sols_.append(sol)\n","        elif (sol[offset] < en[offset]) & (sol[offset+1] <= en[offset+1]):\n","            if sol not in sols_:\n","                sols_.append(sol)\n","        else:\n","            if (sol[offset] < en[offset]) | (sol[offset+1] < en[offset+1]):\n","                if sol not in sols_:\n","                    sols_.append(sol)\n","            if en not in sols_:\n","                sols_.append(en)\n","    return sols_\n","\n","# basic grid serach for fing the efficient clustering\n","def find_clustering(df_htsz, df_celz):\n","    birch_threshold = 0.005\n","    branching_factor = 10\n","    div_ = 99999.0\n","    div1_ = 99999.0\n","    div3_ = 99999.0\n","    k_cluster_ = 10\n","    n_partition_ = 5\n","    w_cluster_ = 0.5\n","    z_true_cluster_ = 0\n","    obj_val_ = 1.0\n","    df_ = pd.DataFrame()\n","\n","    solutions = []\n","\n","    # Number of clusters\n","    for k_cluster in [i*2 for i in reversed(range(1,20))]:\n","        # Weights for clustering\n","        for w_cluster in [i*0.1 for i in range(0,11)]:\n","            df_htszz = df_htsz.drop(columns=['Distance'])\n","            df_celzz = df_celz.drop(columns=['Distance'])\n","            df_hts_ = df_htszz[att_Z].drop_duplicates()\n","            df_cel_ = df_celzz[att_Z].drop_duplicates()\n","            df_clustering = pd.concat([df_cel_, df_hts_]).drop_duplicates()\n","            clusters = clustering(df_clustering, birch_threshold, branching_factor, k_cluster, w_cluster)\n","\n","            df_htszz = pd.merge(df_htszz, clusters, on=att_Z)\n","            df_celzz = pd.merge(df_celzz, clusters, on=att_Z)\n","\n","            # Number of bins/categories (i.e., +2 to convert to the actual number of distance bins)\n","            for n_partition in range(1,13):\n","                add_distance(df_htszz, n_partition)\n","                add_distance(df_celzz, n_partition)\n","\n","                df_htszzz = df_htszz[att_X + att_Y + att_Z  + att_C + ['Prob']]\n","                df_celzzz = df_celzz[att_Y + att_Z  + att_C + ['Prob']]\n","\n","                df_hts = df_htszzz[att_Y + att_C + ['Prob']]\n","                df_cel = df_celzzz[att_Y + att_C + ['Prob']]\n","\n","                df_hts['Prob_hts'] = df_hts.groupby(att_Y + att_C)['Prob'].transform('sum')\n","                df_hts = df_hts.drop(columns=['Prob'])\n","                df_hts = df_hts.drop_duplicates()\n","                df_cel['Prob_cel'] = df_cel.groupby(att_Y + att_C)['Prob'].transform('sum')\n","                df_cel = df_cel.drop(columns=['Prob'])\n","                df_cel = df_cel.drop_duplicates()\n","\n","                df = pd.merge(df_cel, df_hts, on=att_Y + att_C)\n","                df2 = pd.merge(df_cel, df_hts, how='outer', on=att_Y + att_C).reset_index(drop=True)\n","                df2 = df2.replace(np.nan, 0)\n","                z_true_cluster = len(df)\n","                p_cluster = df['Prob_cel'].sum()\n","                div1 = np.sum(js_div(df2['Prob_hts'], df2['Prob_cel']))\n","\n","                df_celzz_ = df_celzzz[att_Z + att_Y + att_C + ['Prob']]\n","                df_celzz_['Prob_cel'] = df_celzz_.groupby(att_Z + att_Y + att_C)['Prob'].transform('sum')\n","                df_celzz_ = df_celzz_.drop(columns=['Prob'])\n","                df_celzz_ = df_celzz_.drop_duplicates()\n","                div2 = calculate_entropy(df_celzz_)\n","\n","                epsilon = 0.98\n","                if (p_cluster > epsilon):\n","                    sol = [k_cluster,\n","                           n_partition,\n","                           w_cluster,\n","                           z_true_cluster,\n","                           div1,\n","                           div2,\n","                           p_cluster]\n","                    solutions_ = find_prareto(solutions, sol)\n","                    solutions = solutions_\n","\n","    return solutions\n","\n","# Get the parameters for clustering\n","solutions = find_clustering(df_htsz, df_celz)\n","df = pd.DataFrame(solutions, columns=list('abcdefg'))\n","df[['ee']] = (df[['e']]-df['e'].min())/[max(0.00001,df['e'].max() - df['e'].min())]\n","df[['ff']] = (df[['f']]-df['f'].min())/[max(0.00001,df['f'].max() - df['f'].min())]\n","df['r'] = df['ee'] + df['ff']\n","\n","df = df.sort_values(by=['r']).iloc[0]\n","result = df[['a', 'b', 'c', 'd']].to_list()\n","k_cluster = result[0]\n","w_cluster = result[2]\n","n_partition = result[1]\n","\n","# Show the parameters\n","print('Optimized hyperparameters:')\n","print('k_cluster: ' + str(k_cluster))\n","print('w_cluster: ' + str(w_cluster))\n","print('n_partition: ' + str(n_partition))\n"]},{"cell_type":"markdown","metadata":{"id":"QuJAbGcAv3wn"},"source":["# Cluster-based data fusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fu1ynjgPv17X"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","from sklearn.cluster import Birch\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","import pyomo.environ as pyo\n","from pyomo.environ import *\n","from scipy.special import rel_entr, kl_div\n","import math\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Attributes\n","att_X = ['Gender', 'Age', 'Homeincome', 'NumHH']\n","att_Y = ['Distance']\n","att_Z = ['Home_X', 'Home_Y' , 'Work_X', 'Work_Y']\n","att_C = ['Cluster']\n","att_HC = ['Home_Hcode', 'Work_Hcode']\n","\n","# JS divergence\n","def js_div(p, q):\n","    h = 0.5*np.add(p,q)\n","    return 0.5*rel_entr(p, h) + 0.5*rel_entr(q, h)\n","\n","# Clustering\n","def clustering(df, birch_threshold, bf, K, w):\n","    w_ = [w, w, 1-w, 1-w]\n","    X = df * w_\n","    data = preprocessing.MinMaxScaler().fit_transform(X)\n","    brc = Birch(threshold=birch_threshold, branching_factor=bf, n_clusters=K)\n","    brc.fit(data)\n","    labels = brc.labels_\n","    pred = brc.predict(data)\n","    df['Cluster'] = pred\n","    clusters = np.unique(pred)\n","    return df.drop_duplicates()\n","\n","\n","# Update auxilary attributes\n","def add_distance(df, N):\n","    lon1 = np.radians(df['Home_X'])\n","    lon2 = np.radians(df['Work_X'])\n","    lat1 = np.radians(df['Home_Y'])\n","    lat2 = np.radians(df['Work_Y'])\n","    dlon = lon2 - lon1\n","    dlat = lat2 - lat1\n","    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n","    df['Distance'] = np.ceil(6371*2 * np.arcsin(np.sqrt(a)))\n","    if (N > 1):\n","        bin_dis = [i*int(12/N)  for i in range(N+1)] + [25,99999]\n","        label_dis = [i for i in range(1,len(bin_dis)-1)] + [99]\n","    else:\n","        bin_dis = [0,99999]\n","        label_dis = [1]\n","    df['Distance'] = pd.cut(df['Distance'], bins=bin_dis, labels=label_dis, include_lowest=True)\n","\n","# Subproblem P_{op_fusion}\n","def prob_XYC_optim(df_hts_XYC, df_cel_YC):\n","    df_hts_XYC_opt = pd.DataFrame()\n","    cluster_loss = 0\n","    print('Subproblem ', len(df_cel_YC))\n","    for index, row in df_cel_YC.iterrows():\n","        df_hts_XYC_ = df_hts_XYC[(df_hts_XYC['Distance'] == row['Distance']) &\n","                                (df_hts_XYC['Cluster'] == row['Cluster'])].reset_index(drop=True)\n","        prob_YC = row['Prob_YC_cel']\n","        N = len(df_hts_XYC_)\n","        obj_val = 0\n","        if N > 0:\n","            model = pyo.ConcreteModel()\n","            model.prob = pyo.Var(range(N), initialize = prob_YC/N, bounds = (0, prob_YC))\n","            model.OBJ = pyo.Objective(expr = np.sum(js_div(df_hts_XYC_['Prob_XYC_hts'],\n","                                        pd.Series(model.prob.extract_values()))), sense = pyo.minimize)\n","            model.const = pyo.Constraint(expr = sum(model.prob[n] for n in range(N)) == prob_YC)\n","            opt_ = pyo.SolverFactory('ipopt')\n","            opt_.solve(model)\n","            obj_val += pyo.value(model.OBJ)\n","            df_hts_XYC_['Prob_XYC_opt'] = pd.Series(model.prob.extract_values())\n","            df_hts_XYC_ = df_hts_XYC_.drop(columns=['Prob_XYC_hts'])\n","            df_hts_XYC_opt = pd.concat([df_hts_XYC_opt, df_hts_XYC_])\n","        else: cluster_loss += prob_YC\n","\n","    df_hts_XYC_opt = df_hts_XYC_opt[df_hts_XYC_opt['Prob_XYC_opt'] > 0]\n","    df_hts_XYC_opt[['Prob_XYC_opt']] = df_hts_XYC_opt[['Prob_XYC_opt']] / df_hts_XYC_opt[['Prob_XYC_opt']].sum()\n","    return cluster_loss, df_hts_XYC_opt\n","\n","# Calculate the fused distribution\n","def prob_XYZ(df_hts_, df_cel_, clusters):\n","    df_hts = pd.merge(df_hts_, clusters, on=att_Z)\n","    df_cel = pd.merge(df_cel_, clusters, on=att_Z)\n","\n","    df_cel_YZC = df_cel[att_Y + att_HC + att_Z + att_C + ['Prob']]\n","    df_cel_YZC.rename(columns = {'Prob':'Prob_YZC_cel'}, inplace = True)\n","\n","    df_cel_YC = df_cel_YZC[att_Y + att_C + ['Prob_YZC_cel']]\n","    df_cel_YC['Prob_YC_cel'] = df_cel_YC.groupby(att_Y + att_C)['Prob_YZC_cel'].transform('sum')\n","    df_cel_YC = df_cel_YC.drop(columns=['Prob_YZC_cel'])\n","    df_cel_YC = df_cel_YC.drop_duplicates()\n","\n","    df_hts_XYC = df_hts[att_X + att_Y + att_C + ['Prob']]\n","    df_hts_XYC['Prob'] = df_hts_XYC.groupby(att_X + att_Y + att_C)['Prob'].transform('sum')\n","    df_hts_XYC = df_hts_XYC.drop_duplicates()\n","    df_hts_XYC.rename(columns = {'Prob':'Prob_XYC_hts'}, inplace = True)\n","\n","    # Subproblem P_{op_fusion}\n","    cluster_loss, df_hts_XYC_opt =  prob_XYC_optim(df_hts_XYC, df_cel_YC)\n","    print('Finish df_hts_XYC_opt')\n","    df_hts_XYC_opt = reduce_mem_usage(df_hts_XYC_opt)\n","\n","    df_merge_XYZ = pd.merge(df_hts_XYC_opt, df_cel_YZC, on=att_Y + att_C)\n","    df_merge_XYZ = pd.merge(df_merge_XYZ, df_cel_YC, on=att_Y + att_C)\n","    df_merge_XYZ['Prob_XYZ'] = df_merge_XYZ['Prob_XYC_opt'] * df_merge_XYZ['Prob_YZC_cel'] / df_merge_XYZ['Prob_YC_cel']\n","    df_merge_XYZ = df_merge_XYZ.drop(columns=['Prob_XYC_opt', 'Prob_YZC_cel', 'Prob_YC_cel'])\n","    df_merge_XYZ[['Prob_XYZ']] = df_merge_XYZ[['Prob_XYZ']]/df_merge_XYZ[['Prob_XYZ']].sum()\n","    print('Prob_XYZ ', df_merge_XYZ[['Prob_XYZ']].sum())\n","\n","    df_merge_XYZ = reduce_mem_usage(df_merge_XYZ)\n","    df_merge_XYZ.to_csv('output/P_XYZ_sim.csv', index = False)\n","    print(df_merge_XYZ.info())\n","    del df_merge_XYZ\n","    print('Clustering loss', cluster_loss)\n","\n","#####################################################################\n","\n","birch_threshold = 0.005\n","branching_factor = 10\n","\n","# Initial clustering parameters\n","k_cluster = 32\n","n_partition = 2\n","w_cluster = 1.0\n","\n","df_celK = pd.read_csv(project_path + 'data/data_cel_sim.csv')\n","df_htsK = pd.read_csv(project_path + 'data/data_hts_sim.csv')\n","\n","df_htsK = df_htsK.drop(columns=['Distance'])\n","df_celK = df_celK.drop(columns=['Distance'])\n","add_distance(df_htsK, n_partition)\n","add_distance(df_celK, n_partition)\n","df_htsK = reduce_mem_usage(df_htsK)\n","df_celK = reduce_mem_usage(df_celK)\n","\n","df_celK = df_celK[att_Y + att_HC + att_Z + ['Prob']]\n","df_celK['Prob'] = df_celK.groupby(att_Y + att_HC + att_Z)['Prob'].transform('sum')\n","df_celK = df_celK.drop_duplicates()\n","\n","df_htsK_ = df_htsK[att_Z].drop_duplicates()\n","df_celK_ = df_celK[att_Z].drop_duplicates()\n","df_spatial_clustering = pd.concat([df_celK_, df_htsK_]).drop_duplicates()\n","\n","# Subproblem P_{clustering}\n","clusters = clustering(df_spatial_clustering, birch_threshold,branching_factor, k_cluster, w_cluster)\n","print('Complete clustering...')\n","\n","# Subproblem P_{op_fusion}\n","prob_XYZ(df_htsK, df_celK, clusters)"]},{"cell_type":"markdown","metadata":{"id":"xI8ayPHeNr8H"},"source":["# Simulation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4QvSMUf5Btn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","import math\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Attributes\n","att_X = ['Gender', 'Age', 'Homeincome', 'NumHH']\n","att_Z = ['Distance','Home_X', 'Home_Y' , 'Work_X', 'Work_Y']\n","att_C = ['Cluster']\n","att_HC = ['Home_Hcode', 'Work_Hcode']\n","\n","# Get index from the true distribution for validation only\n","df_cel = pd.read_csv(project_path + 'data/data_true_sim.csv')\n","df_cel = df_cel[att_HC].reset_index()\n","df_cel['Indx'] = df_cel.index\n","df_cel = df_cel.drop(columns=['index'])\n","print(len(df_cel))\n","\n","# Get the fused distribution\n","df_fus = pd.read_csv(project_path + 'output/P_XYZ_sim.csv')\n","df_fus = df_fus[att_X + att_Z + att_HC + ['Prob_XYZ']]\n","df_fus = reduce_mem_usage(df_fus)\n","\n","# Chunk size for simulation\n","size = 1000\n","list_df_cel = [df_cel[i:i+size] for i in range(0,df_cel.shape[0],size)]\n","Z = math.ceil(len(df_cel)/size)\n","print('no. of chunks', Z)\n","df_final = pd.DataFrame()\n","for z in range(Z):\n","    df_fus_ = pd.merge(df_fus, list_df_cel[z], on= att_HC)\n","    if (len(df_fus_) > 0):\n","        df_fus_[['Prob_XYZ_']] = df_fus_[['Prob_XYZ']] / df_fus_[['Prob_XYZ']].sum()\n","        df_fus_ = df_fus_.reset_index()\n","        df_fus_['Indx2'] = df_fus_.index\n","        NN = int(4*len(df_fus_))\n","        pop = list(range(0, len(df_fus_)))\n","        prob = df_fus_['Prob_XYZ_'].to_numpy()\n","        samples = np.random.choice(pop, size=NN, replace=True, p=prob)\n","        df = pd.DataFrame({'Indx2': samples})\n","        df_fus_ = pd.merge(df, df_fus_, how='left', on=['Indx2'])\n","        df_fus_ = df_fus_.drop_duplicates(subset= ['Indx'], keep='first')\n","        df_fus_ = df_fus_[['Indx'] + att_X + att_HC + att_Z + ['Prob_XYZ']]\n","        df_fus_ = reduce_mem_usage(df_fus_)\n","        df_final = pd.concat([df_final, df_fus_])\n","        del df_fus_\n","\n","df_final = reduce_mem_usage(df_final)\n","print(df_final.info())\n","# Save the simulation result to file\n","df_final.to_csv(project_path + 'output/sample_sim.csv', index=False)"]},{"cell_type":"markdown","source":["# Fesibility, heterogenity, divergence"],"metadata":{"id":"4gbdR9mxGZx0"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.special import rel_entr, kl_div\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def js_div(p, q):\n","    h = 0.5*np.add(p,q)\n","    return 0.5*rel_entr(p, h) + 0.5*rel_entr(q, h)\n","\n","df_true_ = pd.read_csv(project_path + 'data/data_true_sim.csv')\n","df_fus_ = pd.read_csv(project_path + 'output/sample_sim.csv')\n","\n","# Attribute combinations\n","X = [['Gender', 'Age', 'Homeincome'], ['Gender', 'Age', 'NumHH'],  ['Gender', 'Homeincome', 'NumHH'], ['Age', 'Homeincome', 'NumHH']]\n","Z = [['Home_Hcode'], ['Work_Hcode']]\n","att = X[1] + Z[1]\n","\n","df_fus = df_fus_[att]\n","df_fus['Numbers'] = 1.0\n","df_fus['Numbers'] = df_fus.groupby(att)['Numbers'].transform('sum')\n","df_fus = df_fus.drop_duplicates()\n","df_fus[['Prob_fus']] = df_fus[['Numbers']] / df_fus[['Numbers']].sum()\n","df_fus = df_fus.drop(columns=['Numbers'])\n","\n","df_true = df_true_[att]\n","df_true['Numbers'] = 1.0\n","df_true['Numbers'] = df_true.groupby(att)['Numbers'].transform('sum')\n","df_true = df_true.drop_duplicates()\n","df_true[['Prob_true']] = df_true[['Numbers']] / df_true[['Numbers']].sum()\n","df_true = df_true.drop(columns=['Numbers'])\n","\n","df0 = pd.merge(df_true, df_fus, how='outer', on=att)\n","df0 = df0.replace(np.nan, 0)\n","divg = np.sum(js_div(df0['Prob_true'], df0['Prob_fus']))\n","\n","df_true__ = df_true[att]\n","df_fus__ = df_fus[att]\n","\n","df1 = pd.merge(df_fus, df_true__.drop_duplicates(), on=att)\n","precision = df1[['Prob_fus']].sum()[0]\n","df2 = pd.merge(df_true, df_fus__.drop_duplicates(), on=att)\n","recal = df2[['Prob_true']].sum()[0]\n","\n","print('feasibility (precision):', precision)\n","print('diversity (recal):', recal)\n","print('divergence:', divg)\n"],"metadata":{"id":"bB05wNKVGhKJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732678073527,"user_tz":-480,"elapsed":505,"user":{"displayName":"Khoa Vo","userId":"00040010443663681368"}},"outputId":"07e13e41-4839-4913-e896-3dd46d11a5c4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["feasibility (precision): 0.8208552138034508\n","diversity (recal): 0.7836000000000001\n","divergence: 0.1751403631231393\n"]}]},{"cell_type":"markdown","metadata":{"id":"OM-0fngunEal"},"source":["# Ploting sociodemographics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xH7pAD5G7q4I"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def statistics_gen(df_, att, s):\n","    NN = len(df_)\n","    df = df_[att]\n","    df['Numbers'] = 1.0\n","    df['Numbers'] = df.groupby(att)['Numbers'].transform('sum')\n","    df = df.drop_duplicates().reset_index(drop=True)\n","    df[['Pop_' + s]] = df[['Numbers']] / NN\n","    df = df.drop(columns=['Numbers'])\n","    return df\n","\n","def statistics_gen2(df_, att, s):\n","    df = df_[att + ['Prob_XYZ']]\n","    df['Prob_XYZ'] = df.groupby(att)['Prob_XYZ'].transform('sum')\n","    df = df.drop_duplicates().reset_index(drop=True)\n","    df[['Pop_' + s]] = df[['Prob_XYZ']]\n","    df = df.drop(columns=['Prob_XYZ'])\n","    return df\n","\n","def statistics(df_, att, s):\n","    df = df_[att + ['Prob']]\n","    df['Prob'] = df.groupby(att)['Prob'].transform('sum')\n","    df = df.drop_duplicates().reset_index(drop=True)\n","    df[['Pop_' + s]] = df[['Prob']]\n","    df = df.drop(columns=['Prob'])\n","    return df\n","\n","df_gen = pd.read_csv(project_path + 'output/sample_sim.csv')\n","df_hts = pd.read_csv(project_path + 'data/data_hts_sim.csv')\n","df_true = pd.read_csv(project_path + 'data/data_true_sim.csv')\n","\n","df_gen_Age = statistics_gen(df_gen,['Age'],'gen')\n","df_gen_Gender = statistics_gen(df_gen,['Gender'],'gen')\n","df_gen_Homeincome = statistics_gen(df_gen,['Homeincome'],'gen')\n","df_gen_NumHH = statistics_gen(df_gen,['NumHH'],'gen')\n","\n","df_hts_Age = statistics(df_hts,['Age'],'hts')\n","df_hts_Gender = statistics(df_hts,['Gender'],'hts')\n","df_hts_Homeincome = statistics(df_hts,['Homeincome'],'hts')\n","df_hts_NumHH = statistics(df_hts,['NumHH'],'hts')\n","\n","df_true_Age = statistics_gen(df_true,['Age'],'true')\n","df_true_Gender = statistics_gen(df_true,['Gender'],'true')\n","df_true_Homeincome = statistics_gen(df_true,['Homeincome'],'true')\n","df_true_NumHH = statistics_gen(df_true,['NumHH'],'true')\n","\n","df_Age = pd.merge(df_gen_Age, df_hts_Age, on=['Age'])\n","df_Age = pd.merge(df_Age, df_true_Age, on=['Age'])\n","\n","df_Gender = pd.merge(df_gen_Gender, df_hts_Gender, on=['Gender'])\n","df_Gender = pd.merge(df_Gender, df_true_Gender, on=['Gender'])\n","\n","df_Homeincome = pd.merge(df_gen_Homeincome, df_hts_Homeincome, on=['Homeincome'])\n","df_Homeincome = pd.merge(df_Homeincome, df_true_Homeincome, on=['Homeincome'])\n","\n","df_NumHH = pd.merge(df_gen_NumHH, df_hts_NumHH, on=['NumHH'])\n","df_NumHH = pd.merge(df_NumHH, df_true_NumHH, on=['NumHH'])\n","\n","fig_x = 3\n","fig_y = 1.7\n","col_map = {'Pop_gen':'Generated', 'Pop_hts':'HTS', 'Pop_true':'Ground truth'}\n","lengends = ['HTS','Generated', 'Ground truth']\n","colors = ['darkcyan','tab:red', 'navajowhite', 'springgreen']\n","\n","df_Homeincome.rename(columns = col_map, inplace = True)\n","df_Homeincome = df_Homeincome.sort_values(by=['Homeincome']).reset_index(drop=True)\n","df_Homeincome.index = [1,2,3,4,5,6]\n","df_Homeincome[lengends].plot.bar(rot=0,\n","            color=colors,\n","            figsize=(fig_x,fig_y),\n","            xlabel='Household income',\n","            legend=False,\n","            ylabel='Proportion'\n","            )\n","\n","df_NumHH.rename(columns = col_map, inplace = True)\n","df_NumHH = df_NumHH.sort_values(by=['NumHH']).reset_index(drop=True)\n","df_NumHH.index = [1,2,3,4,5,6]\n","df_NumHH[lengends].plot.bar(rot=0,\n","            color=colors,\n","            figsize=(fig_x,fig_y),\n","            xlabel='Household size', legend=False,\n","            ylabel='Proportion'\n","            )\n","\n","df_Age.rename(columns = col_map, inplace = True)\n","df_Age = df_Age.sort_values(by=['Age']).reset_index(drop=True)\n","df_Age.index = [1,2,3,4,5]\n","df_Age[lengends].plot.bar(rot=0,\n","            color=colors,\n","            figsize=(fig_x,fig_y),\n","            xlabel='Age', legend=False,\n","            ylabel='Proportion'\n","            )\n","\n","df_Gender.rename(columns = col_map, inplace = True)\n","df_Gender = df_Gender.sort_values(by=['Gender']).reset_index(drop=True)\n","df_Gender.index = [1,2]\n","df_Gender[lengends].plot.bar(rot=0,\n","            color=colors,\n","            figsize=(fig_x,fig_y),\n","            xlabel='Gender', legend=True,\n","            ylabel='Proportion'\n","            )\n","plt.legend(loc='lower right')\n"]},{"cell_type":"markdown","metadata":{"id":"9pROpLBKJ8y4"},"source":["# Ploting home-work locations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmQJzC_bsgX4"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","from matplotlib import pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","df_fus = pd.read_csv(project_path + 'output/sample_sim.csv')\n","df_true = pd.read_csv(project_path + 'data/data_true_sim.csv')\n","df_hts = pd.read_csv(project_path + 'data/data_hts_sim.csv')\n","\n","def home_work(df_fus, df_hts, df_true, hw):\n","    N_fus = len(df_fus)\n","    home_fus = df_fus[[hw+'_Hcode']]\n","    home_fus['Numbers'] = 1.0\n","    home_fus['Numbers'] = home_fus.groupby([hw+'_Hcode'])['Numbers'].transform('sum')\n","    home_fus = home_fus.drop_duplicates()\n","    home_fus[['Prob_fus']] = home_fus[['Numbers']] /N_fus\n","    home_fus = home_fus.drop(columns=['Numbers'])\n","\n","    home_hts = df_hts[[hw+'_Hcode', 'Prob']]\n","    home_hts['Prob'] = home_hts.groupby(hw+'_Hcode')['Prob'].transform('sum')\n","    home_hts = home_hts.drop_duplicates().reset_index(drop=True)\n","    home_hts[['Prob_hts']] = home_hts[['Prob']]\n","    home_hts = home_hts.drop(columns=['Prob'])\n","\n","    N_true = len(df_true)\n","    home_true = df_true[[hw+'_Hcode']]\n","    home_true['Numbers'] = 1.0\n","    home_true['Numbers'] = home_true.groupby([hw+'_Hcode'])['Numbers'].transform('sum')\n","    home_true = home_true.drop_duplicates()\n","    home_true[['Prob_true']] = home_true[['Numbers']] /N_fus\n","    home_true = home_true.drop(columns=['Numbers'])\n","\n","    ss = 'Population'\n","    if hw == 'Home':\n","        ss = 'Population'\n","    elif hw =='Work':\n","        ss = 'Number_of_employees'\n","\n","    home = pd.merge(home_fus, home_hts,  how='outer', on=[hw+'_Hcode'])\n","    home = pd.merge(home, home_true,  how='outer', on=[hw+'_Hcode'])\n","    home = home.replace(np.nan, 0)\n","    home = home[['Prob_true', 'Prob_fus', 'Prob_hts']]\n","    home = home.sort_values(by=['Prob_true'], ascending=True).reset_index()\n","    home['Indx'] = home.index\n","    home = home.drop(columns=['index'])\n","    return home\n","\n","def work_home_plot(df, hw):\n","    fig_x = 3\n","    fig_y = 1.7\n","    col_map = {'Pop_gen':'Generated', 'Pop_true':'Ground truth', 'Pop_hts':'HTS'}\n","    lengends = ['Generated', 'HTS', 'Ground truth']\n","    colors = ['tab:red', 'darkcyan', 'navajowhite', 'springgreen']\n","    markers = ['o', 'x', '^']\n","\n","    ax = plt.gca()\n","    df.plot(kind='scatter', x='Indx', y='Prob_hts', marker = markers[1], alpha=1, ax = ax, s=7, c=colors[1], linewidth=0.5)\n","    df.plot(kind='scatter', x='Indx', y='Prob_fus', figsize=(fig_x,fig_y), alpha=1, marker=markers[0], ax = ax, s=5, c=colors[0], linewidth=0.5)\n","    df.plot(kind='scatter', x='Indx', y='Prob_true',marker = markers[2], alpha=1, ax = ax, s=3, c=colors[2], linewidth=0.5)\n","\n","    ss = ''\n","    plt.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n","    ax.set_xlabel('Location index ' + ss)\n","    ss = 'Prop. of residents'\n","    if hw =='Work':\n","        ss = 'Prop. of employees'\n","    ax.set_ylabel(ss)\n","\n","home__ = home_work(df_fus, df_hts, df_true,'Home')\n","work__ = home_work(df_fus, df_hts, df_true,'Work')\n","\n","work_home_plot(home__, 'Home')\n","#work_home_plot(work__, 'Work')"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.20"}},"nbformat":4,"nbformat_minor":0}